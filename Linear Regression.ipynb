{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bronze-grill",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-colleague",
   "metadata": {},
   "source": [
    "Linear regression is one of the simpliest algorithm to predict continous values. Here we will look into,\n",
    " - why the linear regression algorithm  is used to predict continous values\n",
    " - why the LMS loss function is used and what is the movtivation for using this\n",
    " - How the gradient descent actuallu work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-police",
   "metadata": {},
   "source": [
    "To look into more in linear regression lets understand it using some examples,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-fireplace",
   "metadata": {},
   "source": [
    "| Living area (feet<sup>2</sup>)|#bedrooms|Price(1000$s)|\n",
    "| ---| ---|---|\n",
    "| 2104 |3|400|\n",
    "|1600|3|330|\n",
    "|2400|3|369|\n",
    "|1416|2|232|\n",
    "|3000|4|540|\n",
    "|..|..|..|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-chicago",
   "metadata": {},
   "source": [
    "Here the input data is 2-Dimensional data. For example $ x^i_{1} $ is the first feature(living area) and $ x^i_{2}$ is the second feature (bedrooms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-clear",
   "metadata": {},
   "source": [
    "To perform supervised learning, we must decide how we’re going to represent functions/hypotheses h in a computer. As an initial choice, let’s say we decide to approximate y as a linear function of x:\n",
    "\n",
    "$$ h_{\\theta}(x) = \\theta_{0} + \\theta_{1} x_{1} + \\theta_{2} x_{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-season",
   "metadata": {},
   "source": [
    "Here, the $\\theta_{i} $ are the parameters (also called **weights**) parameterizing the space of linear functions mapping from X to Y. When there is no risk of\n",
    "confusion, we will drop the θ subscript in $h_{\\theta(x)}$, and write it more simply as h(x). To simplify our notation, we also introduce the convention of letting\n",
    "x0 = 1 (this is the intercept term), so that,\n",
    "                  $$ h(x) = \\sum_{i=0}^{d} \\theta_{i} x_{i} == \\theta^Tx, $$\n",
    "                  \n",
    "where on the right-hand side above we are viewing θ and x both as vectors,\n",
    "and here d is the number of input variables (not counting x0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-model",
   "metadata": {},
   "source": [
    "#### Learning paramters $\\theta$ or cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-newman",
   "metadata": {},
   "source": [
    "Now, given a training set, how do we pick, or learn, the parameters θ? One reasonable method seems to be to make h(x) close to y, at least for the training examples we have. To formalize this, we will define a function that measures, for each value of the θ’s, how close the $h(x^{(i)})’s$ are to the corresponding y(i) ’s, We define the **cost function**,\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{n} (h_{\\theta}(x(x^i) - y^{(i)})^2 $$\n",
    "\n",
    "\n",
    "this is also called **ordinary least squares regression model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-hindu",
   "metadata": {},
   "source": [
    "We want to choose $\\theta$ so as to minimize the $J(\\theta)$, for this we will study below,\n",
    " - **Least Mean Square (LMS)**      - Minimizes the $j(\\theta)$ using gradient descent\n",
    " - **The normal equations**         - Minimize J by explicitly taking its derivatives with respect to the $\\theta_{j’s}$\n",
    " - **Probabilistic interpretation** - Using probabilistic assumptions, under which least-squares regression is derived as a very natural algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-exchange",
   "metadata": {},
   "source": [
    "Reference;-\n",
    "1. CS229 Andrew NG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-expert",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-accounting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
